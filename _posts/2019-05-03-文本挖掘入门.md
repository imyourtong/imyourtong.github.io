---
layout:     post
title:      文本挖掘入门
subtitle:   关于词袋（BOW）和TF-IDF
date:       2019-05-03
author:     tongchen
header-img: img/home-bg-art.jpg
catalog: true
tags:
    - python
    
---


## 词袋模型（Bag of Words）

首先，词袋模型是什么？可以用下面这张图来简单表示：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2o1toeu56j30rb0kb7kd.jpg)

在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现。这种假设虽然对自然语言进行了简化，便于模型化。

假定在有些情况下是不合理的，例如在新闻个性化推荐中，采用Bag of words的模型就会出现问题。例如用户甲对“南京醉酒驾车事故”这个短语很感兴趣，采用bag of words忽略了顺序和句法，则认为用户甲对“南京”、“醉酒”、“驾车”和“事故”感兴趣，因此可能推荐出和“南京”，“公交车”，“事故”相关的新闻，这显然是不合理的。

以“苏宁易购是国内著名的B2C电商之一”为例，这句话分词后是这样的：`苏宁易购/是/国内/著名/的/B2C/电商/之一`可以看作是一个这样的词袋：

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2o1yyzfavj30eg0d9q4v.jpg)

但计算机是没有办法读懂这些汉字的，所以我们要把这些词语转化为计算机可读懂的二进制语言。

![](https://ws3.sinaimg.cn/large/006tNc79ly1g2o1zipryzj30ds0cit8x.jpg)

针对词袋模型，我以下面这四句话为例阐述怎样用代码实现:

"I come to China to travel", 
"This is a car polupar in China",          
"I love tea and Apple ",   
"The work is to write some papers in science"

```
from sklearn.feature_extraction.text import CountVectorizer  
vectorizer=CountVectorizer()
corpus=["I come to China to travel", 
    "This is a car polupar in China",          
    "I love tea and Apple ",   
    "The work is to write some papers in science"] 
print (vectorizer.fit_transform(corpus))
```

将其进行向量化，结果如下：

![](https://ws1.sinaimg.cn/large/006tNc79ly1g2o244m803j30a40mimy7.jpg)

第一个数字代表词所在的句子，第二个数字代表它所在的位置，第三个数字代表出现的次数。

```
print (vectorizer.fit_transform(corpus).toarray())
print (vectorizer.get_feature_names())
```
每个句子用特征向量表示出来是这样的：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2o27lcalqj31js06et9p.jpg)

可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词"I"在英文中是停用词，不参加词频的统计。

由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是稀疏的。在实际应用中一般使用稀疏矩阵来存储。

如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现"come","China"和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的"China"和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。

## TF-IDF

字面上可以理解：当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。

某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2o29qwat0j30nd0cb16z.jpg)

![](https://ws3.sinaimg.cn/large/006tNc79ly1g2o2ak8qy4j30ry03tmy8.jpg)

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2o2atumrhj30lp06bjsq.jpg)

![](https://ws3.sinaimg.cn/large/006tNc79ly1g2o2b4bk6hj30pg05h0wg.jpg)


解释“逆文档频率”：比如语料库有10亿文档，里面都包含“的”字，那么“的”的逆文档频率就是log1，也就是0（这样的词我们叫它“停止词”，文本处理一定要去掉停用词）；比如“原子能”在200万个文档中出现，那么“原子能”的逆文档频率就是8.96。

TF-IDF除了考虑一个词在一篇文章中出现的频率（即在本文中的重要性），其实还在考查一个词预测主题的能力。因为如果一个词在所有文本中都出现了，那么我们很难通过这个词猜到文本在讲什么。

## 政府工作报告文本挖掘

可以参考之前抓取政协报告的代码从国史网爬取1954年-2017年的政府工作报告。

### 读取数据

```
with open('../data/gov_reports1954-2017.txt', 'r') as f:
    reports = f.readlines()
```

### 导入所需要的库
```
%matplotlib inline
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import sys 
import numpy as np
from collections import defaultdict
import statsmodels.api as sm
from wordcloud import WordCloud
import jieba
import matplotlib
import gensim
from gensim import corpora, models, similarities
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

#matplotlib不会每次启动时都重新扫描所有的字体文件并创建字体索引列表，
# 因此在复制完字体文件之后，需要运行下面的语句以重新创建字体索引列表
from matplotlib.font_manager import _rebuild
_rebuild()
#防止中文乱码问题
import matplotlib as mpl
mpl.rcParams['font.sans-serif']=[u'SimHei']
mpl.rcParams['axes.unicode_minus']=False

matplotlib.rc("savefig", dpi=400)

```

### 分词/去停用词

```
import jieba
 
 
# jieba.load_userdict('userdict.txt')
# 创建停用词list
def stopwordslist(filepath):
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
    return stopwords
 
 
# 对句子进行分词
def seg_sentence(sentence):
    sentence_seged = jieba.cut(sentence.strip())
    stopwords = stopwordslist('/Users/user/Desktop/ChinanewsAwards/stopwords.txt')  # 这里加载停用词的路径
    outstr = ''
    for word in sentence_seged:
        if word not in stopwords:
            if word != '\t':
                outstr += word
                outstr += " "
    return outstr
 
 
inputs = open('../data/gov_reports1954-2017.txt', 'r', encoding='utf-8')
outputs = open('../data/gov_reports1954-2017_cut.txt', 'w')
for line in inputs:
    line_seg = seg_sentence(line)  # 这里的返回值是字符串
    outputs.write(line_seg + '\n')
outputs.close()
inputs.close()
```
### 使用TF-IDF提取文本关键词

```
import jieba.analyse
txt = reports[-1]
tf = jieba.analyse.extract_tags(txt, topK=200, withWeight=True)
```

```
u"、".join([i[0] for i in tf[:50]])
```

得到如下结果：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2o2pxugrgj30v2044dhb.jpg)

画图展示2017年的关键词：

```
import pandas as pd

def keywords(index):
    txt = reports[-index]
    tf = jieba.analyse.extract_tags(txt, topK=200, withWeight=True)
    tr = jieba.analyse.textrank(txt,topK=200, withWeight=True)
    tfdata = pd.DataFrame(tf, columns=['word', 'tfidf'])
    trdata = pd.DataFrame(tr, columns=['word', 'textrank'])
    worddata = pd.merge(tfdata, trdata, on='word')
    fig = plt.figure(figsize=(16, 6),facecolor='white')
    plt.plot(worddata.tfidf, worddata.textrank, linestyle='',marker='.')
    for i in range(len(worddata.word)):
        plt.text(worddata.tfidf[i], worddata.textrank[i], worddata.word[i], 
                 fontsize = worddata.textrank[i]*30, 
                 color = 'red', rotation = 0
                )
    plt.title(txt[:4])
    plt.xlabel('Tf-Idf')
    plt.ylabel('TextRank')
    plt.show()
```
```
keywords(1)
```
![](https://ws2.sinaimg.cn/large/006tNc79ly1g2o2r3wzj1j30v60cign3.jpg)

之后我会进一步更新有关word embedding等更fancy的文本挖掘方法。

参考:

- [机器学习：生动理解TF-IDF算法](https://zhuanlan.zhihu.com/p/31197209)
- [文本挖掘预处理之TF-IDF](https://www.cnblogs.com/pinard/p/6693230.html)
- [文本挖掘简介-chengjun](https://nbviewer.jupyter.org/github/computational-class/bigdata/blob/gh-pages/code/10.text_minning_gov_report.ipynb)
