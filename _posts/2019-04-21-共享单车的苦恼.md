---
layout:     post
title:      怎样更好的安排一辆共享单车？
subtitle:   基于pytorch的一次预测训练
date:       2019-04-21
author:     tongchen
header-img: img/post-bg-ioses.jpg
catalog: true
tags:
    - pytorch
    - 预测问题
    
---


## 共享单车的苦恼

共享单车行业蓬勃发展，方便了大家的“最后一公里”。对于我来说，何止是
”最后一公里“的便利，我的出行基本全靠楼下的小蓝车，但也时不时会遇见马上就要上课了，宿舍楼下却一辆小蓝车都没有的尴尬场景。

那么，就引出了共享单车领域的三大难题：

1.究竟什么时间派工人去搬运？

2.应该从哪里搬运到哪里？

3.应该搬运多少辆单车？

不如，我们来做一个【单车预测器】，这也是我在2019人工智能与社会科学项目中运行过的小课题。

## 数据来源

共享单车租赁过程与环境和季节性环境高度相关。 比如说天气情况,降水量、周的天数、季节、日的小时数等都会影响租赁行为。核心数据集由[美国华盛顿首都自行车共享系统](http://capitalbikeshare.com/system-data)提供。数据颗粒度有天和小时，然后提取并加入相应的天气和季节信息。天气信息摘自http://www.freemeteo.com

数据中抽取十天，画图，呈现这样的趋势：

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2b793qw86j310q0howjk.jpg)

## Redication of bike rental count

这是一个回归问题，需要理解梯度下降法和神经元建构。代码如下：

### 导入需要的库

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt import torch
from torch.autograd import Variable
#让输出的图形直接在Notebook中显示 
%matplotlib inline
```

### 导入数据

```
#读取数据到内存中，rides为一个dataframe对象 
data_path = ‘Bike-Sharing-Dataset/hour.csv’
rides = pd.read_csv(data_path) #看看数据长什么样子
rides.head()
```
数据格式如下：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2bdylsmcrj318208edir.jpg)

### 初始化神经网络权重等变量
提取数据库的cnt字段前50条记录，50行1列。

```
counts = rides[‘cnt’][:50]
x = Variable(torch.FloatTensor(np.arange(len(counts), dtype = float)/len(counts)))
y = Variable(torch.FloatTensor(np.array(counts, dtype = float)))
sz = 10
#初始化所有神经网络的权重(weights)和阈值(biases)
weights = Variable(torch.randn(1, sz), requires_grad = True)
biases = Variable(torch.randn(sz), requires_grad = True)
weights2 = Variable(torch.randn(sz, 1), requires_grad = True)

```
神经层构造形式如下：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2bdzkuwlkj30wg0bwjtp.jpg)

### 进行梯度下降迭代
学习率和层数都可以根据自己的指标进行修改。

```
learning_rate = 0.0001 #设置学习率 losses = []
for i in range(1000000):
#从输入层到隐含层的计算
hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz) #将sigmoid函数作用在隐含层的每一个神经元上
hidden = torch.sigmoid(hidden)
#隐含层输出到输出层，计算得到最终预测
predictions = hidden.mm(weights2)
#通过与标签数据y比较，计算误差
loss = torch.mean((predictions - y) ** 2)
if i % 10000 == 0:
print(‘loss:’, loss)
loss.backward() #对损失函数进行梯度反传 #利用上一步计算中得到的weights，biases等梯度信息更新weights或biases中的data数值 weights.data.add_(- learning_rate * weights.grad.data)
biases.data.add_(- learning_rate * biases.grad.data)
weights2.data.add_(- learning_rate * weights2.grad.data)
#梯度清空
weights.grad.data.zero_()
biases.grad.data.zero_()
weights2.grad.data.zero_()
```
拟合结果如下：

![](https://ws3.sinaimg.cn/large/006tNc79ly1g2be719yroj30tg0kwwhf.jpg)

损失函数如下：

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2be7me9n6j30gm0b4gmc.jpg)

损失函数很重要，一定要计算。

### 预测

```
counts_predict = rides['cnt'][50:100]#读取待预测的接下来的50个数据点 #首先对接下来的50个数据点进行选取，注意x应该取51，52，......，100，然后再归一化
x = Variable(torch.FloatTensor((np.arange(len(counts_predict), dtype = float) + len(counts)) / len(counts_predict)))
#读取下50个点的y数值
y = Variable(torch.FloatTensor(np.array(counts_predict, dtype = float)))
# 从输入层到隐含层的计算
hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz)
# 将sigmoid函数作用在隐含层的每一个神经元上 hidden = torch.sigmoid(hidden)
# 隐含层输出到输出层，计算得到最终预测 predictions = hidden.mm(weights2)
# 计算预测数据上的损失函数
loss = torch.mean((predictions - y) ** 2) print(loss)

```

预测结果很糟糕：

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2bearil6wj310c0jq0uk.jpg)

运用梯度下降的回归方法存在着这样几个问题：
1.存在着严重的过拟合现象；
2.采用单一属性（编号）预测未来单车数量效果太差；
3.运行速度很缓慢；
4.需要考虑其他可获得信息：是否工作日、天气情况、风速、湿度等

那么，设计一个多输入的神经网络能否改善结果呢？

## 使用pytorch再次出发

我们仔细来看看数据的意义：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2befop9puj31ee0j4gu5.jpg)

cnt的值受到多变量的影响，因此考虑设计一个多层Neu。首先我们要搞清楚，这不是一个线性的回归问题，这两者的区别在下面这张图中可以清楚的看出来：

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2bejqquczj30xq0bi0uz.jpg)

两种神经网络也存在着结构性的差别：

![](https://ws1.sinaimg.cn/large/006tNc79ly1g2beks5cxmj30z60csaek.jpg)

### 数据预处理：类型变量

如：weekday:1，2，3，4，5，6，0

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2benkap09j30o208qab7.jpg)

### 数据预处理：数值类型变量归一化

如：体表温度

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2bepsa64dj30wi0bm76q.jpg)

### 数据准备

从整个2011-01-01至2012-12-31的数据集中2011-01-01至2012-12-11作为训练集，剩下的作为测试集。将训练集切割成小撮（batch），对每一小撮进行误差计算、反向传播、调整权重。这是一种兼具准确和高效的数据处理方式。

### 建立神经网络
这部分定义了一个`input_size × sz`的第一层权重网络，定义了一个sz尺度的向量biases，定义了一个`sz × 1`的第二层权重网络。

```
input_size = features.shape[1]
output_size = 1
weights1 = Variable(torch.randn([input_size, hidden_size]), requires_grad = True)
biases = Variable(torch.randn([hidden_size]), requires_grad = True)
weights2 = Variable(torch.randn([hidden_size, output_size]), requires_grad = True)
```

这部分代码建立了一个多步操作的神经网络模型，第一步从输入层到隐含层节点作为一个线性运算，输入维度input_size，隐含维度hidden_size；第二步为Sigmoid，作用到每一个隐含层神经元上；第三步又是一个神经元，从隐含到输出，神经元的个数分别为hidden_size和output_size；所有神经元的参数都储存在neu.parameters（）里面了。

```
neu= torch.nn.Sequential( torch.nn.Linear(input_size, hidden_size), torch.nn.Sigmoid(), torch.nn.Linear(hidden_size, output_size),
)
```

### 建立损失函数和优化器

```
cost = torch.nn.MSELoss()
optimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)
```
torch.nn.MSELoss()等价于函数torch.mean((x-y)^2)

### 主要训练循环

```
# 神经网络训练循环 for i in range(2000):
batch_loss = [] #记录每一个撮的损失
# 每128个样本点被划分为一个撮
# start和end分别是提取一个batch数据的起始和终止下标 for start in range(0, len(X), batch_size):
end = start + batch_size if start + batch_size < len(X) else len(X) xx = Variable(torch.FloatTensor(X[start:end]))
yy = Variable(torch.FloatTensor(Y[start:end]))
predict = neu(xx) # 模型预测
loss = cost(predict, yy) # 计算损失函数(均方误差) optimizer.zero_grad() #将优化器存储的那些参数的梯度设置为0 loss.backward() # 开始反向传播，计算所有梯度值 optimizer.step() # 优化器开始运行一步，更新所有的参数 batch_loss.append(loss.data.numpy())
# 每隔100步输出一下损失值(loss) if i % 100==0:
losses.append(np.mean(batch_loss)) print(i, np.mean(batch_loss))
```

运行结果如下：

![](https://ws4.sinaimg.cn/large/006tNc79ly1g2bfoir0opj30zo0hc7bj.jpg)

我们可以看见在22/23/24这几点的预测结果有相当大的偏差，这归结于圣诞节假期的反常模式：

1.24号预测值偏高是因为对节假日抑制单元的抑制不够；

2.由于圣诞节的缘故。22/23号两天的午高峰出行较少，甚至比一般节假日还少。

### 误差解决方法

特殊日期的训练需要更多的数据，或者手工调整权重。



- 参考 [火炬上的深度学习](http://swarma.org/?p=4461)
