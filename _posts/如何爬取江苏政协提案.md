---
layout:     post
title:      如何爬取江苏政协提案？
subtitle:   小白教程No.2
date:       2019-03-10
author:     tongchen
header-img: img/home-bg-art.jpg
catalog: true
tags:
    - pythom
    - html 
  
    
---


## 查看网页源代码

在这里我使用的是谷歌的chrome浏览器，点击“视图-开发者-显示源代码“即可查看网页的源代码。
江苏省政协提案网站是：http://www.jszx.gov.cn/zxta/2019ta/
## 分析
通过对ULR的分析发现，在2017年之前的网页都是使用JS推送的，2017年之前的则没有使用JS推送因此直接抓取即可。
##代码
###导入需要的库

```
import requests
from bs4 import BeautifulSoup
```


###制定抓取规则
```
form_data = {'year':2019,
        'pagenum':1,
        'pagesize':20
}
url = 'http://www.jszx.gov.cn/wcm/zxweb/proposalList.jsp'
content = requests.get(url, form_data)
content.encoding = 'utf-8'
js = content.json()
```
```
dat = js['data']['list']
pagenum = js['data']['pagecount']

```
###开始抓取
```
for i in range(2, pagenum+1):
    print(i)
    form_data['pagenum'] = i
    content = requests.get(url, form_data)
    content.encoding = 'utf-8'
    js = content.json()
    for j in js['data']['list']:
        dat.append(j)
```
```
import pandas as pd
df = pd.DataFrame(dat)
df.head()
```
```
url_base = 'http://www.jszx.gov.cn/wcm/zxweb/proposalInfo.jsp?pkid='
urls = [url_base + i  for i in df['pkid']]
```
```
import sys
def flushPrint(www):
    sys.stdout.write('\r')
    sys.stdout.write('%s' % www)
    sys.stdout.flush()
```
```
text = []
for k, i in enumerate(urls):
    flushPrint(k)
    content = requests.get(i)
    content.encoding = 'utf-8'
    js = content.json()
    js = js['data']['binfo']['_content']
    soup = BeautifulSoup(js, 'html.parser') 
    text.append(soup.text)
```
```
df['content'] = text
```
###写入文件
```
df.to_csv('jszx2019.csv', index = False)   
```
如果顺利的话，就可以得到像这样的一个csv文件，包含提案的全部信息。
![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0xmmsf9rsj31bd0u0hdt.jpg)

- 参考 [chengjun的bigdata](https://github.com/computational-class/bigdata/blob/gh-pages/code/04.PythonCrawlerGovernmentReport.ipynb)
