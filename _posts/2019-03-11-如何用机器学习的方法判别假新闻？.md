---
layout:     post
title:      如何用机器学习的方法判别假新闻？
subtitle:   pytorch项目入门
date:       2019-03-11
author:     tongchen
header-img: img/post-bg-github-cup.jpg
catalog: true
tags:
    - pytorch
    - python
    
---


## Pytorch学习简介

在进行这一部分项目的时候，前期主要根据集智俱乐部张江老师的课程[深度学习与pytorch](http://campus.swarma.org/gcou=10335)进行学习。在2019年寒假中我参加了集智俱乐部在南京大学开设的《计算社会科学：当社会科学遇到人工智能》这门课程，参与了关于机器学习的讨论和实操。

对我非常有帮助的两个github项目是：

[deep-learning-v2-pytorch](https://github.com/imyourtong/deep-learning-v2-pytorch)这个是优达学院一个讲解pytorch课程的参考资料，里面详细介绍了如何安装pytorch、如何配置环境、机器学习背后的逻辑。

[pytorch-tutorial](https://github.com/imyourtong/pytorch-tutorial)这个是一个韩国小哥写的代码，里面有很多小练习，格式规整清晰，方便新手入门实操。

## 如何训练一个神经网络？

这个项目里的数据来自kaggle，里面包含一个已经训练好的数据集和一个未分类的测试数据集。

### 导入程序所需要的程序包

```
#抓取网页内容用的程序包
import json
import requests
import pandas as pd
import string

#PyTorch用的包
import torch
import torch.nn as nn
import torch.optim
from torch.autograd import Variable

#自然语言处理相关的包
import re #正则表达式的包
import jieba #结巴分词包(kaggle这个是英文新闻 ，因此这个包不需用，但在解析中文文本时，结巴分词包必不可少）
from collections import Counter #搜集器，可以让统计词频更简单

#绘图、计算用的程序包
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
```
### 导入训练集和测试集
```	
train=pd.read_csv('newstrain.csv')
test=pd.read_csv('newstest.csv')
```
### 提取训练数据
因为这个数据集过大，个人电脑内存只有8G，因此在这个项目中我只提去了标题部分进行训练。

```
train['total']=train['title']
test['total']=test['title']
```
```
data=train.append(test)
```
我们可以看一下前五个数据长什么样：

```
data.head() #查看前五个
```
![](https://ws3.sinaimg.cn/large/006tKfTcly1g0ytt4pg2qj31jc0co417.jpg)

### 构造真/假新闻训练集
```
true_file=data[data['label']==0]['total'].values
fake_file=data[data['label']==1]['total'].values
```
```
f=[]
for i in fake_file:
    f.append(str(i))
```
```
fake_file=f
```
```
with open('true_file.txt','w') as f:
    data=f.writelines([line+ '\n' for line in true_file])
with open('fake_file.txt','w') as f:
    data=f.writelines([line+ '\n' for line in fake_file])
```
### 英文去标点 
```
def filter_punc(sentence):
    for c in string.punctuation: #去标点 
        sentence = sentence.replace(c, ' ')
    return(sentence)
```

### 分词、建立真/假词典

这部分我借鉴了张江课程中判断京东购物评论是正向还是负向的神经网络代码：

```
#扫描所有的文本，分词、建立词典，分出正向还是负向的评论，is_filter可以过滤是否筛选掉标点符号

def Prepare_data(good_file, bad_file, is_filter = True):
    all_words = [] #存储所有的单词
    pos_sentences = [] #存储正向的评论
    neg_sentences = [] #存储负向的评论
    with open(good_file, 'r') as fr:
        for idx, line in enumerate(fr):
            if is_filter:
                #过滤标点符号
                line = filter_punc(line)
            #分词
            words = line.split(' ')
            if len(words) > 0:
                all_words += words
                pos_sentences.append(words)
    print('{0} 包含 {1} 行, {2} 个词.'.format(good_file, idx+1, len(all_words)))

    count = len(all_words)
    with open(bad_file, 'r') as fr:
        for idx, line in enumerate(fr):
            if is_filter:
                line = filter_punc(line)
            words = line.split(' ')
            if len(words) > 0:
                all_words += words
                neg_sentences.append(words)
    print('{0} 包含 {1} 行, {2} 个词.'.format(bad_file, idx+1, len(all_words)-count))

    #建立词典，diction的每一项为{w:[id, 单词出现次数]}
    diction = {}
    cnt = Counter(all_words)
    for word, freq in cnt.items():
        diction[word] = [len(diction), freq]
    print('字典大小：{}'.format(len(diction)))
    return(pos_sentences, neg_sentences, diction)
    
```
```

def word2index(word, diction):#根据单词返还单词的编码
    if word in diction:
        value = diction[word][0]
    else:
        value = -1
    return(value)

def index2word(index, diction):#根据编码获得单词
    for w,v in diction.items():
        if v[0] == index:
            return(w)
    return(None)

pos_sentences, neg_sentences, diction = Prepare_data('true_file.txt', 'fake_file.txt', is_filter = True)
st = sorted([(v[1], w) for w, v in diction.items()])
st

```
顺利的话这一步的结果是这样的：
![](https://ws2.sinaimg.cn/large/006tKfTcly1g0yufdu2l5j30jy0imwga.jpg)

### 向量化
```
# 输入一个句子和相应的词典，得到这个句子的向量化表示
# 向量的尺寸为词典中词汇的个数，i位置上面的数值为第i个单词在sentence中出现的频率
def sentence2vec(sentence, dictionary):
    vector = np.zeros(len(dictionary))
    for l in sentence:
        vector[l] += 1
    return(1.0 * vector / len(sentence))

# 遍历所有句子，将每一个词映射成编码
dataset = [] #数据集
labels = [] #标签
sentences = [] #原始句子，调试用
# 处理正向评论
for sentence in pos_sentences:
    new_sentence = []
    for l in sentence:
        if l in diction:
            new_sentence.append(word2index(l, diction))
    dataset.append(sentence2vec(new_sentence, diction))
    labels.append(0) #正标签为0
    sentences.append(sentence)

# 处理负向评论
for sentence in neg_sentences:
    new_sentence = []
    for l in sentence:
        if l in diction:
            new_sentence.append(word2index(l, diction))
    dataset.append(sentence2vec(new_sentence, diction))
    labels.append(1) #负标签为1
    sentences.append(sentence)

#打乱所有的数据顺序，形成数据集
# indices为所有数据下标的一个全排列
indices = np.random.permutation(len(dataset))

#重新根据打乱的下标生成数据集dataset，标签集labels，以及对应的原始句子sentences
dataset = [dataset[i] for i in indices]
labels = [labels[i] for i in indices]
sentences = [sentences[i] for i in indices]

#对整个数据集进行划分，分为：训练集、校准集和测试集，其中校准和测试集合的长度都是整个数据集的10分之一
test_size = len(dataset) // 10
train_data = dataset[2 * test_size :]
train_label = labels[2 * test_size :]

valid_data = dataset[: test_size]
valid_label = labels[: test_size]

test_data = dataset[test_size : 2 * test_size]
test_label = labels[test_size : 2 * test_size]
```

### 建立神经网络
```
# 一个简单的前馈神经网络，三层，第一层线性层，加一个非线性ReLU，第二层线性层，中间有10个隐含层神经元

# 输入维度为词典的大小：每一段评论的词袋模型
model = nn.Sequential(
    nn.Linear(len(diction), 10),
    nn.ReLU(),
    nn.Linear(10, 2),
    nn.LogSoftmax(),
)

def rightness(predictions, labels):
    """计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案"""
    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标
    rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量
    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素
```

```
# 损失函数为交叉熵
cost = torch.nn.NLLLoss()
# 优化算法为Adam，可以自动调节学习率
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)
records = []

#循环10个Epoch
losses = []
for epoch in range(10):
    for i, data in enumerate(zip(train_data, train_label)):
        x, y = data
        
        # 需要将输入的数据进行适当的变形，主要是要多出一个batch_size的维度，也即第一个为1的维度
        x = Variable(torch.FloatTensor(x).view(1,-1))
        # x的尺寸：batch_size=1, len_dictionary
        # 标签也要加一层外衣以变成1*1的张量
        y = Variable(torch.LongTensor(np.array([y])))
        # y的尺寸：batch_size=1, 1
        
        # 清空梯度
        optimizer.zero_grad()
        # 模型预测
        predict = model(x)
        # 计算损失函数
        loss = cost(predict, y)
        # 将损失函数数值加入到列表中
        losses.append(loss.data.numpy())
        # 开始进行梯度反传
        loss.backward()
        # 开始对参数进行一步优化
        optimizer.step()
        
        # 每隔3000步，跑一下校验数据集的数据，输出临时结果
        if i % 3000 == 0:
            val_losses = []
            rights = []
            # 在所有校验数据集上实验
            for j, val in enumerate(zip(valid_data, valid_label)):
                x, y = val
                x = Variable(torch.FloatTensor(x).view(1,-1))
                y = Variable(torch.LongTensor(np.array([y])))
                predict = model(x)
                # 调用rightness函数计算准确度
                right = rightness(predict, y)
                rights.append(right)
                loss = cost(predict, y)
                val_losses.append(loss.data.numpy())
                
            # 将校验集合上面的平均准确度计算出来
            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])
            print('第{}轮，训练损失：{:.2f}, 校验损失：{:.2f}, 校验准确率: {:.2f}'.format(epoch, np.mean(losses),
                                                                        np.mean(val_losses), right_ratio))
            records.append([np.mean(losses), np.mean(val_losses), right_ratio])
            
```
在10轮循环之后，准确率达到了0.94.

### 观察每个神经元作用 
```
# 绘制出第二个全链接层的权重大小
# model[2]即提取第2层，网络一共4层，第0层为线性神经元，第1层为ReLU，第2层为第二层神经原链接，第3层为logsoftmax
plt.figure(figsize = (10, 7))
for i in range(model[2].weight.size()[0]):
    #if i == 1:
        weights = model[2].weight[i].data.numpy()
        plt.plot(weights, 'o-', label = i)
plt.legend()
plt.xlabel('Neuron in Hidden Layer')
plt.ylabel('Weights')
```
我的训练结果如下：
![](https://ws3.sinaimg.cn/large/006tKfTcly1g0yulsi4nij30ny0hsq5f.jpg)

### 得到权重大的单词

```
# 将第二层的各个神经元与输入层的链接权重，挑出来最大的权重和最小的权重，并考察每一个权重所对应的单词是什么，把单词打印出来
# model[0]是取出第一层的神经元

# for i in range(len(model[0].weight)):
i=9
print('\n')
print('第{}个神经元'.format(i))
print('max:')
st = sorted([(w,i) for i,w in enumerate(model[0].weight[i].data.numpy())])
for i in range(1, 20):
    word = index2word(st[-i][1],diction)
    print(word)
print('min:')
for i in range(20):
    word = index2word(st[i][1],diction)
    print(word)
```
这个入门pytorch项目到这里就告一段落了，因为配置的原因没有做全文本的尝试，但我做出一些“猜想”：

1.全文本的训练准确度会更高；

2.加入对文章作者的测量，因为写假新闻的可能是同样的一批人，这样的测量可以高效的提升准确度。
